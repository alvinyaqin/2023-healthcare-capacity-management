# Load packages
```{r}
library(tidyverse)
library(lubridate)
library(tensorflow)
library(keras)
library(zoo)
library(tseries)
library(readxl)
library(Metrics)
```

# RS1 L14 lr=0.1
```{r}
# Load data
rs1 <- read_excel("D:/SIAP S.T/Data/COVID/kanudjoso.xlsx")

# Autocorrelation analysis
acf(rs1)
acf(rs1$Actual, lag.max = 40)
lag.plot(rs1$Actual, lag = 14, do.lines = FALSE)

data <- rs1[, 2]
ts_data <- ts(data, frequency = 1)
head(ts_data)

#Data preparation
#Transform data to stationary
diffed = diff(ts_data, differences = 1)
head(diffed)

#Lagged dataset
lag_transform <- function(x, k= 14){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = lag_transform(diffed, 14)
head(supervised)

#Split dataset into training and testing sets
N = nrow(supervised)
n = round(N *0.8, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

#Normalize the data
## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train$x[-c(1:14)]
x_train = Scaled$scaled_train$x[-c((length(Scaled$scaled_train$x)-13):length(Scaled$scaled_train$x))]

y_test = Scaled$scaled_test$x
x_test = Scaled$scaled_test$x[-c((length(Scaled$scaled_test$x)-13):length(Scaled$scaled_test$x))]

## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, 
             batch_input_shape = c(
               batch_size, 
               X_shape2, 
               X_shape3), 
             stateful= TRUE)%>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam(learning_rate=0.1),
  metrics = c("mean_absolute_error")
)

summary(model)

# Fit the model
Epochs = 50   
for(i in 1:Epochs ){model %>% fit(
  x_train, 
  y_train, 
  epochs=1, 
  batch_size=batch_size, 
  verbose=1, 
  shuffle=FALSE)
  model %>% reset_states()
}

# Make predictions
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + ts_data[(n+i)]
     # store
     predictions[i] <- yhat
}

predicted <- as.integer(predictions)


predicted_values <- predicted[1:46]

actual_values <- ts_data[241:286]

mse <- mse(actual_values, predicted_values)
rmse <- rmse(actual_values, predicted_values)

actual_mean <- mean(actual_values)
SST <- sum((actual_values - actual_mean)^2)
SSE <- sum((actual_values - predicted_values)^2)
R_square <- 1 - (SSE / SST)

min_val <- min(actual_values)
max_val <- max(actual_values)

cat(sprintf("MSE: %f\n", mse))
cat(sprintf("RMSE: %f\n", rmse))
cat(sprintf("R-Square: %f\n", R_square))
cat(sprintf("Min value: %f\n", min_val))
cat(sprintf("Max value: %f\n", max_val))

results <- data.frame(Predicted = predicted_values,
                      Actual = actual_values,
                      RSquare = R_square)

print(results)
```

# RS2 L14 lr=0.001
```{r}
# Load data
rs2 <- read_excel("D:/SIAP S.T/Data/COVID/pertamina.xlsx")

# Autocorrelation analysis
acf(rs2)
acf(rs2$Actual, lag.max = 40)
lag.plot(rs2$Actual, lag = 14, do.lines = FALSE)

data <- rs2[, 2]
ts_data <- ts(data, frequency = 1)
head(ts_data)

#Data preparation
#Transform data to stationary
diffed = diff(ts_data, differences = 1)
head(diffed)

#Lagged dataset
lag_transform <- function(x, k= 14){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = lag_transform(diffed, 14)
head(supervised)

#Split dataset into training and testing sets
N = nrow(supervised)
n = round(N *0.8, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

#Normalize the data
## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train$x[-c(1:14)]
x_train = Scaled$scaled_train$x[-c((length(Scaled$scaled_train$x)-13):length(Scaled$scaled_train$x))]

y_test = Scaled$scaled_test$x
x_test = Scaled$scaled_test$x[-c((length(Scaled$scaled_test$x)-13):length(Scaled$scaled_test$x))]

## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, 
             batch_input_shape = c(
               batch_size, 
               X_shape2, 
               X_shape3), 
             stateful= TRUE)%>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam(learning_rate=0.001),
  metrics = c("mean_absolute_error")
)

summary(model)

# Fit the model
Epochs = 50   
for(i in 1:Epochs ){model %>% fit(
  x_train, 
  y_train, 
  epochs=1, 
  batch_size=batch_size, 
  verbose=1, 
  shuffle=FALSE)
  model %>% reset_states()
}

# Make predictions
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + ts_data[(n+i)]
     # store
     predictions[i] <- yhat
}

predicted <- as.integer(predictions)


predicted_values <- predicted[1:46]

actual_values <- ts_data[241:286]

mse <- mse(actual_values, predicted_values)
rmse <- rmse(actual_values, predicted_values)

actual_mean <- mean(actual_values)
SST <- sum((actual_values - actual_mean)^2)
SSE <- sum((actual_values - predicted_values)^2)
R_square <- 1 - (SSE / SST)

min_val <- min(actual_values)
max_val <- max(actual_values)

cat(sprintf("MSE: %f\n", mse))
cat(sprintf("RMSE: %f\n", rmse))
cat(sprintf("R-Square: %f\n", R_square))
cat(sprintf("Min value: %f\n", min_val))
cat(sprintf("Max value: %f\n", max_val))

results <- data.frame(Predicted = predicted_values,
                      Actual = actual_values,
                      RSquare = R_square)

print(results)
```

# RS3 L14 lr=0.001
```{r}
# Load data
rs3 <- read_excel("D:/SIAP S.T/Data/COVID/beriman.xlsx")

# Autocorrelation analysis
acf(rs3)
acf(rs3$Actual, lag.max = 40)
lag.plot(rs3$Actual, lag = 14, do.lines = FALSE)

data <- rs3[, 2]
ts_data <- ts(data, frequency = 1)
head(ts_data)

#Data preparation
#Transform data to stationary
diffed = diff(ts_data, differences = 1)
head(diffed)

#Lagged dataset
lag_transform <- function(x, k= 14){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = lag_transform(diffed, 14)
head(supervised)

#Split dataset into training and testing sets
N = nrow(supervised)
n = round(N *0.8, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

#Normalize the data
## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train$x[-c(1:14)]
x_train = Scaled$scaled_train$x[-c((length(Scaled$scaled_train$x)-13):length(Scaled$scaled_train$x))]

y_test = Scaled$scaled_test$x
x_test = Scaled$scaled_test$x[-c((length(Scaled$scaled_test$x)-13):length(Scaled$scaled_test$x))]

## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, 
             batch_input_shape = c(
               batch_size, 
               X_shape2, 
               X_shape3), 
             stateful= TRUE)%>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam(learning_rate=0.001),
  metrics = c("mean_absolute_error")
)

summary(model)

# Fit the model
Epochs = 50   
for(i in 1:Epochs ){model %>% fit(
  x_train, 
  y_train, 
  epochs=1, 
  batch_size=batch_size, 
  verbose=1, 
  shuffle=FALSE)
  model %>% reset_states()
}

# Make predictions
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + ts_data[(n+i)]
     # store
     predictions[i] <- yhat
}

predicted <- as.integer(predictions)


predicted_values <- predicted[1:46]

actual_values <- ts_data[241:286]

mse <- mse(actual_values, predicted_values)
rmse <- rmse(actual_values, predicted_values)

actual_mean <- mean(actual_values)
SST <- sum((actual_values - actual_mean)^2)
SSE <- sum((actual_values - predicted_values)^2)
R_square <- 1 - (SSE / SST)

min_val <- min(actual_values)
max_val <- max(actual_values)

cat(sprintf("MSE: %f\n", mse))
cat(sprintf("RMSE: %f\n", rmse))
cat(sprintf("R-Square: %f\n", R_square))
cat(sprintf("Min value: %f\n", min_val))
cat(sprintf("Max value: %f\n", max_val))

results <- data.frame(Predicted = predicted_values,
                      Actual = actual_values,
                      RSquare = R_square)

print(results)
```

# RS4 L14 lr=0.001
```{r}
# Load data
rs4 <- read_excel("D:/SIAP S.T/Data/COVID/siloam.xlsx")

# Autocorrelation analysis
acf(rs4)
acf(rs4$Actual, lag.max = 40)
lag.plot(rs4$Actual, lag = 14, do.lines = FALSE)

data <- rs4[, 2]
ts_data <- ts(data, frequency = 1)
head(ts_data)

#Data preparation
#Transform data to stationary
diffed = diff(ts_data, differences = 1)
head(diffed)

#Lagged dataset
lag_transform <- function(x, k= 14){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = lag_transform(diffed, 14)
head(supervised)

#Split dataset into training and testing sets
N = nrow(supervised)
n = round(N *0.8, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

#Normalize the data
## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train$x[-c(1:14)]
x_train = Scaled$scaled_train$x[-c((length(Scaled$scaled_train$x)-13):length(Scaled$scaled_train$x))]

y_test = Scaled$scaled_test$x
x_test = Scaled$scaled_test$x[-c((length(Scaled$scaled_test$x)-13):length(Scaled$scaled_test$x))]

## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, 
             batch_input_shape = c(
               batch_size, 
               X_shape2, 
               X_shape3), 
             stateful= TRUE)%>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam(learning_rate=0.001),
  metrics = c("mean_absolute_error")
)

summary(model)

# Fit the model
Epochs = 50   
for(i in 1:Epochs ){model %>% fit(
  x_train, 
  y_train, 
  epochs=1, 
  batch_size=batch_size, 
  verbose=1, 
  shuffle=FALSE)
  model %>% reset_states()
}

# Make predictions
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + ts_data[(n+i)]
     # store
     predictions[i] <- yhat
}

predicted <- as.integer(predictions)


predicted_values <- predicted[1:46]

actual_values <- ts_data[241:286]

mse <- mse(actual_values, predicted_values)
rmse <- rmse(actual_values, predicted_values)

actual_mean <- mean(actual_values)
SST <- sum((actual_values - actual_mean)^2)
SSE <- sum((actual_values - predicted_values)^2)
R_square <- 1 - (SSE / SST)

min_val <- min(actual_values)
max_val <- max(actual_values)

cat(sprintf("MSE: %f\n", mse))
cat(sprintf("RMSE: %f\n", rmse))
cat(sprintf("R-Square: %f\n", R_square))
cat(sprintf("Min value: %f\n", min_val))
cat(sprintf("Max value: %f\n", max_val))

results <- data.frame(Predicted = predicted_values,
                      Actual = actual_values,
                      RSquare = R_square)

print(results)
```

# RS5 L14 lr=0.1
```{r}
# Load data
rs5 <- read_excel("D:/SIAP S.T/Data/COVID/hardjanto.xlsx")

# Autocorrelation analysis
acf(rs5)
acf(rs5$Actual, lag.max = 40)
lag.plot(rs5$Actual, lag = 14, do.lines = FALSE)

data <- rs5[, 2]
ts_data <- ts(data, frequency = 1)
head(ts_data)

#Data preparation
#Transform data to stationary
diffed = diff(ts_data, differences = 1)
head(diffed)

#Lagged dataset
lag_transform <- function(x, k= 14){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = lag_transform(diffed, 14)
head(supervised)

#Split dataset into training and testing sets
N = nrow(supervised)
n = round(N *0.8, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

#Normalize the data
## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train$x[-c(1:14)]
x_train = Scaled$scaled_train$x[-c((length(Scaled$scaled_train$x)-13):length(Scaled$scaled_train$x))]

y_test = Scaled$scaled_test$x
x_test = Scaled$scaled_test$x[-c((length(Scaled$scaled_test$x)-13):length(Scaled$scaled_test$x))]

## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, 
             batch_input_shape = c(
               batch_size, 
               X_shape2, 
               X_shape3), 
             stateful= TRUE)%>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam(learning_rate=0.1),
  metrics = c("mean_absolute_error")
)

summary(model)

# Fit the model
Epochs = 50   
for(i in 1:Epochs ){model %>% fit(
  x_train, 
  y_train, 
  epochs=1, 
  batch_size=batch_size, 
  verbose=1, 
  shuffle=FALSE)
  model %>% reset_states()
}

# Make predictions
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + ts_data[(n+i)]
     # store
     predictions[i] <- yhat
}

predicted <- as.integer(predictions)


predicted_values <- predicted[1:46]

actual_values <- ts_data[241:286]

mse <- mse(actual_values, predicted_values)
rmse <- rmse(actual_values, predicted_values)

actual_mean <- mean(actual_values)
SST <- sum((actual_values - actual_mean)^2)
SSE <- sum((actual_values - predicted_values)^2)
R_square <- 1 - (SSE / SST)

min_val <- min(actual_values)
max_val <- max(actual_values)

cat(sprintf("MSE: %f\n", mse))
cat(sprintf("RMSE: %f\n", rmse))
cat(sprintf("R-Square: %f\n", R_square))
cat(sprintf("Min value: %f\n", min_val))
cat(sprintf("Max value: %f\n", max_val))

results <- data.frame(Predicted = predicted_values,
                      Actual = actual_values,
                      RSquare = R_square)

print(results)
```

# RS6 L14 lr=0.01
```{r}
# Load data
rs6 <- read_excel("D:/SIAP S.T/Data/COVID/bhayangkara.xlsx")

# Autocorrelation analysis
acf(rs6)
acf(rs6$Actual, lag.max = 40)
lag.plot(rs6$Actual, lag = 14, do.lines = FALSE)

data <- rs6[, 2]
ts_data <- ts(data, frequency = 1)
head(ts_data)

#Data preparation
#Transform data to stationary
diffed = diff(ts_data, differences = 1)
head(diffed)

#Lagged dataset
lag_transform <- function(x, k= 14){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = lag_transform(diffed, 14)
head(supervised)

#Split dataset into training and testing sets
N = nrow(supervised)
n = round(N *0.8, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

#Normalize the data
## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train$x[-c(1:14)]
x_train = Scaled$scaled_train$x[-c((length(Scaled$scaled_train$x)-13):length(Scaled$scaled_train$x))]

y_test = Scaled$scaled_test$x
x_test = Scaled$scaled_test$x[-c((length(Scaled$scaled_test$x)-13):length(Scaled$scaled_test$x))]

## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

#Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, 
             batch_input_shape = c(
               batch_size, 
               X_shape2, 
               X_shape3), 
             stateful= TRUE)%>%
  layer_dense(units = 1)

# Compile the model
model %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam(learning_rate=0.01),
  metrics = c("mean_absolute_error")
)

summary(model)

# Fit the model
Epochs = 50   
for(i in 1:Epochs ){model %>% fit(
  x_train, 
  y_train, 
  epochs=1, 
  batch_size=batch_size, 
  verbose=1, 
  shuffle=FALSE)
  model %>% reset_states()
}

# Make predictions
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + ts_data[(n+i)]
     # store
     predictions[i] <- yhat
}

predicted <- as.integer(predictions)


predicted_values <- predicted[1:46]

actual_values <- ts_data[241:286]

mse <- mse(actual_values, predicted_values)
rmse <- rmse(actual_values, predicted_values)

actual_mean <- mean(actual_values)
SST <- sum((actual_values - actual_mean)^2)
SSE <- sum((actual_values - predicted_values)^2)
R_square <- 1 - (SSE / SST)

min_val <- min(actual_values)
max_val <- max(actual_values)

cat(sprintf("MSE: %f\n", mse))
cat(sprintf("RMSE: %f\n", rmse))
cat(sprintf("R-Square: %f\n", R_square))
cat(sprintf("Min value: %f\n", min_val))
cat(sprintf("Max value: %f\n", max_val))

results <- data.frame(Predicted = predicted_values,
                      Actual = actual_values,
                      RSquare = R_square)

print(results)
```

